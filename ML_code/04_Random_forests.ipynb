{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Since Decision Trees have a tendency to overfit, Random Forests were introduced to reduce this risk by building multiple decision trees and combining their outputs. The key idea behind random forests is to use several trees, each of which might overfit on some part of the data but, by considering numerous instances we can reduce the level of overfitting by averaging the results.\n",
    "\n",
    "To introduce randomness in the tree growing process 2 methods are regularly used: **selecting random data points** (samples) and **selecting random features** in which to split a node. \n",
    "\n",
    "The random sample selection usually considers **bootstrapping**, that is randomly selecting a part (usually 2/3) of the whole training data set with replacement. This way we can get many samples from the original training data set and create different trees (the training in fact stops when we reach the number of trees we want to create).\n",
    "\n",
    "The random feature selection does not look for the best split for each node, but takes a random subset of features and looks for the best fit for one of these features. The number of feature to select is a key parameter. Usually, selecting a high number of features means that the trees to create will be rather similar to each other, while a low number of features will generate trees quite different between each other (and each tree will need a larger depth to fit the data).\n",
    "\n",
    "To test Classification cases, the method considers using a 'vote' witht he prediction of all the generated trees to define a final prediction (in regression cases, we use the average of the predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#Random forest class defines the number of trees to grow and compare \n",
    "#as well as the attributes a DecisionTreeClass has\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees = 10, min_samples_split = 2, max_depth = 50, n_features = None):\n",
    "        self.n_trees = n_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.trees = [] #this will store the trees we grow as an array of trees\n",
    "    \n",
    "    #fit function for all the trees. We use the fit method from DecisionTreeClass to grow each tree\n",
    "    def fit(self, X, y):\n",
    "        self.trees = [] #initiate the trees array to empty\n",
    "        #creates n_trees random trees \n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTreeClass(min_samples_split = self.min_samples_split, \n",
    "                                     max_depth = self.max_depth,\n",
    "                                     n_features = self.n_features)\n",
    "            X_sample, y_sample = self._bootstrap(X,y) #randomly select a sub-set of the training data with replacement\n",
    "            tree.fit(X_sample, y_sample) #grow the tree with the subset of samples\n",
    "            self.trees.append(tree) #we add the newly created tree to the trees array\n",
    "    \n",
    "    \n",
    "    #randomly select 2/3 of the total training data for each random tree\n",
    "    def _bootstrap(self, X, y):\n",
    "        n_train_set = X.shape[0]\n",
    "        samples_indx = np.random.choice(n_train_set, int(n_train_set * 2/3), replace = True)\n",
    "        return X[samples_indx], y[samples_indx]\n",
    "    \n",
    "    #define prediction from a test sample. Notice that we use the predict method from DesicionTreeClass\n",
    "    def predict(self, X_sample):\n",
    "        \"\"\"this returns an a set of 'n_trees' number of arrays, \n",
    "        each with the predictions for every test sample in X_sample\"\"\"\n",
    "        predics = np.array([tree.predict(X_sample) for tree in self.trees])\n",
    "        \n",
    "        #with this function we create len(X_sample) number of arrays, \n",
    "        #each with the predictions for one value of X_sample so we can easily evaluate the most voted result \n",
    "        preds_by_tree = self.restruct_array(predics)\n",
    "        \n",
    "        #takes the most 'voted' prediction for each result and returns an array with it\n",
    "        results_ = []\n",
    "        for result in preds_by_tree:\n",
    "            count = Counter(result)\n",
    "            results_.append(count.most_common(1)[0][0])\n",
    "            \n",
    "        return results_\n",
    "        \n",
    "        #this function returns\n",
    "    def restruct_array(self, arrays):\n",
    "        n = len(arrays)\n",
    "        m = len(arrays[0])  # Assuming all arrays have the same length\n",
    "\n",
    "        result = [[] for _ in range(m)]\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                result[i].append(arrays[j][i])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we'll also need the code for Decision Trees (though we could import it as a library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, index = None, threshold = None, left_branch = None, right_branch = None, info_gain = None, *, value = None):\n",
    "        \n",
    "        #decision nodes \n",
    "        self.feature_index = index\n",
    "        self.threshold = threshold\n",
    "        self.left = left_branch\n",
    "        self.right = right_branch\n",
    "        self.info_gain = info_gain\n",
    "        self.value = value #Notice that value is only defined for leaf/terminal nodes \n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTreeClass():\n",
    "    def __init__(self, min_samples_split = 2, max_depth = 50, n_features = None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #get characteristics of the [remaining] training data set (consider the min between the defined features and the given features)\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X,y)\n",
    "        \n",
    "    #this is the key method. It recursively create tree (with initial depth = 0)\n",
    "    def _grow_tree(self, X, y, depth = 0):\n",
    "        n_samples = X.shape[0]\n",
    "        n_rem_labels = len(np.unique(y)) #records the different labels for the remaining samples in the training set\n",
    "        \n",
    "        #check stopping criteria: size of tree, only 1 label (entropy = 0)\n",
    "        if (depth >= self.max_depth or n_rem_labels == 1 or n_samples <= self.min_samples_split):\n",
    "            counter = Counter(y)\n",
    "            leaf_value = counter.most_common(1)[0][0] #this is the label of the most common element in this terminal node\n",
    "            return Node(value = leaf_value)\n",
    "        \n",
    "        #find the best split for the node\n",
    "        #1st we randomly select the features. Notice that we can also select just a portion of the feature, not nec. all of them\n",
    "        feat_indexes = np.random.choice(range(self.n_features), self.n_features, replace = False) \n",
    "        best_feat, best_thrsh  = self._best_split(X, y, feat_indexes)\n",
    "        \n",
    "        #create children nodes recursively slicing the sample\n",
    "        left_indexes, right_indexes = self._split(X[:, best_feat], best_thrsh)\n",
    "        left = self._grow_tree(X[left_indexes], y[left_indexes], depth + 1)\n",
    "        right = self._grow_tree(X[right_indexes], y[right_indexes], depth + 1)\n",
    "        \n",
    "        return Node(best_feat, best_thrsh, left, right)\n",
    "        \n",
    "        \n",
    "    #this function evaluates all possible splits and defines the one \n",
    "    #that maximises the information gain\n",
    "    def _best_split(self, X, y, feat_indexes):\n",
    "        best_gain = -10\n",
    "        split_index, split_thrshld = None, None\n",
    "        \n",
    "        #we define all possible splits\n",
    "        for f_indx in feat_indexes:\n",
    "            X_col = X[:, f_indx]\n",
    "            thrshld = np.unique(X_col)\n",
    "            \n",
    "            for thr in thrshld:\n",
    "                inf_gain = self._inf_gain(y, X_col,thr)\n",
    "                \n",
    "                #update the register for the best gain split\n",
    "                if inf_gain > best_gain:\n",
    "                    best_gain = inf_gain \n",
    "                    split_index = f_indx\n",
    "                    split_thrshld = thr\n",
    "        \n",
    "        return split_index, split_thrshld\n",
    "        \n",
    "    #calculates and returns information gain for a given threshold to split on a node\n",
    "    def _inf_gain(self, y, X_col, thr):\n",
    "        #parent's entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "        \n",
    "        #define children's branches by indixes\n",
    "        left_branch, right_branch = self._split(X_col, thr)\n",
    "        \n",
    "        #if there's no more splitting to do, inf. gain is null\n",
    "        if len(left_branch) == 0 or len(right_branch) == 0:\n",
    "            return 0\n",
    "        \n",
    "        #define children's entropy\n",
    "        n_left, n_right = len(left_branch), len(right_branch)\n",
    "        entropy_left, entropy_right = self._entropy(y[left_branch]), self._entropy(y[right_branch])\n",
    "        children_entropy = (n_left/len(y))*entropy_left + (n_right/len(y))*entropy_right\n",
    "        \n",
    "        #calculate information gain\n",
    "        inf_gain = parent_entropy - children_entropy\n",
    "        return inf_gain\n",
    "        \n",
    "    #returns arrays with left-right branches according to a threshold value\n",
    "    def _split(self, X_col, split_thr):\n",
    "        left_child = np.argwhere(X_col <= split_thr).flatten() #we use flatten() to have just one array\n",
    "        right_child = np.argwhere(X_col > split_thr).flatten() #we use flatten() to have just one array\n",
    "        return left_child, right_child\n",
    "    \n",
    "    #calculate entropy for a given sample set of labels\n",
    "    def _entropy(self, y):\n",
    "        labels, labels_count = np.unique(y, return_counts = True)\n",
    "        prop_label = labels_count / len(y)\n",
    "        return -np.sum([p*np.log(p) for p in prop_label if p > 0])\n",
    "    \n",
    "    #return an array of the predictions for an array of values    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._gothrough_tree(x, self.root) for x in X])\n",
    "        \n",
    "\n",
    "    #recursively determine the end_node for a given value and return its most prominent label(prediction)\n",
    "    def _gothrough_tree(self, x, node):\n",
    "\n",
    "        #for terminal node, return its value\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._gothrough_tree(x, node.left)\n",
    "        return self._gothrough_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,let's evaluate the model with the same data set we used for the Decision Trees algorithm. Notice that with 30 trees we get an accuracy of 96.7%, slightly higher than what we got with just Decision Trees (app 93%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "#importing a data set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#evaluate accuracy\n",
    "def accuracy(y_test, y_prediction):\n",
    "    return np.sum(y_test == y_prediction) / len(y_test)\n",
    "\n",
    "col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'type']\n",
    "data = pd.read_csv(\"data/Iris.csv\", skiprows = 1, header = None, names = col_names)\n",
    "data.head(10)\n",
    "\n",
    "data.shape\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values#.reshape(-1,1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=41)\n",
    "\n",
    "rf = RandomForest(n_trees = 30)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "\n",
    "print(accuracy(predictions, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
