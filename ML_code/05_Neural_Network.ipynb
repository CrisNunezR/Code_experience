{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/neural_networks_1.png\" alt=\"Image source: https://tikz.net/neural_networks/\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks (also known as Deep Learning) are a family of algorithms inspired by a simplified version of how the brain works, with \"neurons\" receiving, processing and forwarding information to other \"neurons\".\n",
    "\n",
    "As the image above shows, neural networks (NN) simplify the workings of the brain by using \"layers\" of neurons that use information from the previous \"layer\", processing the information and rendering the result to the next \"layer\" in the network.\n",
    "\n",
    "Each neuron of a given layer is 'connected' to all the neurons (or inputs in the case of the 1st layer) of the previous layer, as shown by the arrows on the image above.\n",
    "\n",
    "The features (X) are usually referred to as the \"**input layer**\" and defined as $a^{[0]}$. The last layer is named \"**output layer**\" while the layers in between are named \"**hidden layers**\" and denoted as $a^{[i]}$, where i defines the number of the layer. Considering this, the neuron j on layer i is denoted as $a_j^{[i]}$.\n",
    "\n",
    "\n",
    "The basic unit in a NN is a neuron or **perceptron**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/perceptron.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single neuron is very similar to a Logistic Regression, with an input that can have several features fed to a function using weights $w_i$ and a bias b. The result of this operation is then passed to an **activation function** that determines the output of the neuron/perceptron.\n",
    "\n",
    "There are many activation functions, each one with their own merits, however, the most frequently used are the following ones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/activation_functions.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the input to the activation function is always *f*$_{w,b}(X)=W•X+b$, which in the images above is presented as either 'z' or 'y'. \n",
    "\n",
    "For this explanaition, I will use the **sigmoid function** given that we already saw it for the Logistic Regression and it's derivative is easy to interpret.\n",
    "\n",
    "As can be gathered from the images of the activation functions above, they have a dual purpose: \n",
    "- break the linear relationship between the layers\n",
    "- accelerate or reduce the iteration change depending on the form of the function(we'll see this later on)\n",
    "\n",
    "The 1st effect is particularly important since, if the activation function were linear functions (that is a(f(X)) = kf(x), with k a scalar), it can be demonstrated that the whole NN will simply be a linear function of the input and would not perform any better than a regular Logistic Regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "Now, let's look at a network with 4 layers: 1 input layer (with 3 features), 2 hidden layers (both with 2 neurons) and one output layer with one neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/NN_layers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = $[x_1, x_2, x_3]$ represents the input features and $\\hat{y}$ the output of the system.\n",
    "\n",
    "Notice how each neuron is connected with all the neurons (or features) from the previous layer. Remember that the notations are:\n",
    "- $a_{i}^{[j]}$: is the activation function for the $i^{th}$ neuron on layer j\n",
    "- $w_{i,k}^{[j]}$: is the weight for the connection between neuron (or feature, for layer 0) *i* on layer (j-1) to the neuron *k* on layer (j), and \n",
    "- $W^{[j]}$ would be the vector including all weights for the $j^{th}$ layer.\n",
    "\n",
    "Also, the terminal node or output layer defines the output using some rule or threshold (commonly, if $a^{[output\\_layer]}>0.5$ -> $\\hat{y}=1$, otherwise $\\hat{y}=0$).\n",
    "\n",
    "With this, the equations to define the value of output $\\hat{y}\\approx a_{1}^{[3]}$ are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/NN_fwd_prop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where **$\\sigma$ is the activation function**:\n",
    "\n",
    "$\\sigma(W^{[j]}•X + b^{[j]}) =  \\frac{1}{1+\\exp^{-(W^{[j]}•X + b^{[j]})}}$, where j defines the layer.\n",
    "\n",
    "Notice that each neuron could have its own activation function, however, it's customary that all neurons in a layer use the same activation function, though functions can change between layers (it is in fact common to use Rectified Linear Unit -ReLU- for the hidden layers and a sigmoid or no function for the output layer).\n",
    "\n",
    "Also notice that each neuron's output is a function of all the previous neurons and inputs (via the activation functions of each previous layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process, in which we calculated the output $\\hat{y}$ for the model from the matrix of input features X is called **Forward Propagation**. \n",
    "\n",
    "Similarly to what we did in the Logistic Regression, once we have the predicted output value $a^{[output\\_layer]}$, we estimate the **Loss Function** (for individual samples) and the **Cost Function** for the whole -training- data set (notice that here we will use the output $a^{[]}$ instead of the binary value $\\hat{y}$ since we will have more information to approximate the optimal parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a classification algorithm, we can have a **Loss Function** of the form:\n",
    "\n",
    "$$ Loss Function(a^{[output\\_layer]}, y) = - (y log(a^{[output\\_layer]})  + (1-y)log(1-a^{[output\\_layer]})) $$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Notice that, if we were using a regression model instead of a classification one, we would probably use a loss function of the form: \n",
    "\n",
    "$ Loss(a^{[output\\_layer]}, y) = (a^{[output\\_layer]} - y)^2 $\n",
    "\n",
    "</div>\n",
    "\n",
    "Thus, for the **Cost Function** (that considers the aggregated loss function for all the 'm' samples in the set) we have:\n",
    "\n",
    "$$ Cost Function = J(W,b) = \\frac{1}{m} \\sum_{i=1}^{m}Loss Function(a^{[output\\_layer](i)}, y^{(i)})$$\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Notice that we are using **addition instead of multiplication** for the Cost Function even though to take the aggreate probabilities for all the samples in the training set we should be multiplying, not adding the Loss Functions.\n",
    "\n",
    "This is caused by the 'ingeneous mathematical trick' of using the log-likelihood instead of working directly with the probabilities/likelihood, as it is more convenient to sum the log-likelihoods and minimising (or maximising) the likelihood is equivalent to minimising (maximising) the log-likelihoods for the objective of finding the best parameters W (weights) and b (bias).\n",
    "\n",
    "In other words, if we want to maximise the probability of events 'X' and 'Y', we need to calculate:\n",
    "\n",
    "$Max(Prob(x)*Prob(Y))$\n",
    "\n",
    "However, we can also use the log of the multiplication and arrieve at the same optimal point, with the benefit of working with additions instead of multiplications:\n",
    "\n",
    "$Max(log(Prob(X)*Prob(Y))) = Max(log(Prob(X)+Prob(Y)))$\n",
    "\n",
    "We will not demonstrate it, but it is also equivalent to max the log of the sum and the sum itself to the objective of finding the optimal parameters:\n",
    "\n",
    "$Max(log(Prob(X)*Prob(Y))) = Max(log(Prob(X)+Prob(Y))) \\equiv Max(Prob(X)+Prob(Y)) $\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous steps we have a prediction $(\\hat{y})$ for a given input (features set X) and some parameters (W and b), but we need to find the optimal parameters that minimise the Cost Function $(J_{(W,b)}(X))$, that is, the parameters that yield the minimum difference between the predicted output ($\\hat{y}$) and the actual output ('y', which we know from the training data).\n",
    "\n",
    "Conceptually, the idea is the same as the one we used with the Logistic Regression algorithm (Gradient Descent), that is, we take the partial derivative of the Cost Function with respect to the parameters (W,b) and we iterate modifying the parameters in small steps in the direction that minimises the Cost Function (defined by the partial derivatives). In other words, we change the parameters with these equations:\n",
    "\n",
    "$ (1) \\quad W_{t+1} = W_{t} - \\theta \\frac{\\partial{J(W,b)}}{\\partial{w}} $\n",
    "\n",
    "$ (2) \\quad b_{t+1} = b_{t} - \\theta \\frac{\\partial{J(W,b)}}{\\partial{b}} $\n",
    "\n",
    "(Where t defines the iteration, so $W_{t+1}$ is the value of W for the iteration after $W_{t}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the problem is that we have many layers and finding out how to change W and b is not straight forward. This is where the idea of 'back propagation' comes in. Let's see it in detail (incidentally, this youtube video has a good and visual explanation as well and I will be using some of its approach here: https://www.youtube.com/watch?v=tIeHLnjs5U8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Simple_Network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this simplified Network, we can see that:\n",
    "\n",
    "$a^{[k]} =$ *_f_*$(w^{[k]}a^{[k-1]}+b^{[k]}) $, where *f*() is an activation function and we'll use the sigmoid ($\\sigma$) for now.\n",
    "\n",
    "For notation simplification, we will also use $z^{[k]}_{(w,b)}=w^{[k]}a^{[k-1]}+b^{[k]}$, so we have:\n",
    "\n",
    "$(3) \\quad a^{[k]} = \\sigma(z^{[k]}_{(w,b)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for $\\delta{J}/\\delta{w}$ and $\\delta{J}/\\delta{b}$, using the **chain rule** for derivatives we have, for the output layer:\n",
    "\n",
    "$ (4) \\quad \\frac{\\delta{J}}{\\delta{w^{[k]}}} = \\frac{\\delta{J}}{\\delta{a^{[k]}}} \n",
    "\\frac{\\delta{a^{[k]}}}{\\delta{z^{[k]}}} \n",
    "\\frac{\\delta{z^{[k]}}}{\\delta{w^{[k]}}} $\n",
    "\n",
    "$ (5) \\quad \\frac{\\delta{J}}{\\delta{b^{[k]}}} = \\frac{\\delta{J}}{\\delta{a^{[k]}}} \n",
    "\\frac{\\delta{a^{[k]}}}{\\delta{z^{[k]}}} \n",
    "\\frac{\\delta{z^{[k]}}}{\\delta{b^{[k]}}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work out the partial derivatives for $w^{[k]}$ and a single sample:\n",
    "\n",
    "$ \\frac{\\delta{z^{[k]}}}{\\delta{w^{[k]}}} = a^{[k-1]}$\n",
    "\n",
    "$ \\frac{\\delta{a^{[k]}}}{\\delta{z^{[k]}}} = \\sigma'(z^{[k]}_{(w,b)})$,  notice that this derivative depends on the activation function we use\n",
    "\n",
    "$ \\frac{\\delta{J}}{\\delta{a^{[k]}}} = \\frac{-y}{a^{[k]}} + \\frac{1-y}{1-a^{[k]}}$, this derivative is also different between classification and regression algorithms.\n",
    "\n",
    "So, for a single sample in the training set, we have:\n",
    "\n",
    "$ \\frac{\\delta{J}}{\\delta{w^{[k]}}} = a^{[k-1]} \\sigma'(z^{[k]}_{(w,b)}) [\\frac{-y}{a^{[k]}} + \\frac{1-y}{1-a^{[k]}}] $\n",
    "\n",
    "But, remembering that $ a^{[k]} = \\sigma(z^{[k]}_{(w,b)}) $ and that $ \\sigma '(z) = \\sigma(z) \\sigma(1-z)$, we can simplify the previous equation to:\n",
    "\n",
    "$ (6) \\quad \\frac{\\delta{J}}{\\delta{w^{[k]}}} = a^{[k-1]} (a^{[k]} - y)$ for the sigmoid activation.\n",
    "\n",
    "Now, for $ \\frac{\\delta J}{\\delta b}$, we can see that the only element that changes is $ \\frac{\\delta{z^{[k]}}}{\\delta{w^{[k]}}}$, which becomes:\n",
    "\n",
    "$ \\frac{\\delta{z^{[k]}}}{\\delta{b^{[k]}}} = 1 $, so:\n",
    "\n",
    "$ (7) \\quad \\frac{\\delta{J}}{\\delta{b^{[k]}}} = (a^{[k]} - y) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Notice that, to calculate the value of the cost for the whole sample set, we need to average the results:\n",
    "\n",
    "\n",
    "$$ \\frac{\\delta{J}}{\\delta{w^{[k]}}} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\delta J^{(i)}}{\\delta w^{[k]}} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "For regression models, we usually have:\n",
    "\n",
    "$$ Loss\\_Function = (a^{[output\\_layer]}-y)^2  $$\n",
    "\n",
    "This means that the derivative of the cost function over $a^{[k]}$ is:\n",
    "\n",
    "$ \\quad \\frac{\\delta{J}}{\\delta{a^{[k]}}} = 2(a^{[k]}-y)$\n",
    "\n",
    "Also, if we use the sigmoid function, we have:\n",
    "\n",
    "$ \\quad \\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "So, the derivative over z is:\n",
    "\n",
    "$ \\quad \\frac{\\delta \\sigma(z)}{\\delta z} = \\sigma(z) (1- \\sigma(z))$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remamber from equiations (1) and (2), we calculated the partial derivatives to find the adjusted values for the parameters. But equation (6) shows us that the value depends on the activation function ($a^{[k-1]}$). \n",
    "\n",
    "This is why this is called \"back propagation\", since we continue passing on the value backwards applying the same concept until we reach the input ($a^{[0]}$).\n",
    "\n",
    "For example, for [k-1] we have:\n",
    "\n",
    "$ \\quad \\quad \\frac{\\delta{J}}{\\delta{w^{[k-1]}}} = a^{[k-2]} (a^{[k-1]} - y) $\n",
    "\n",
    "$ \\quad \\quad \\frac{\\delta{J}}{\\delta{b^{[k-1]}}} =(a^{[k-1]} - y) $\n",
    "\n",
    "And so, we continue propagating the calculation until we arrive at the intput vector ($a^{[0]}$).\n",
    "\n",
    "Once we have this, we slightly modify the values of the parameters (W,b) according to equations (1) and (2) with a given \"learning rate\" ($\\theta$) and continue iterating until a terminal condition is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Notice that the activation function could change between layers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a following file we'll look at the code that applies these concepts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
